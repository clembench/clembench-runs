<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>-, clemscore</th>
      <th>all, Average % Played</th>
      <th>all, Average Quality Score</th>
      <th>imagegame, % Played</th>
      <th>imagegame, Quality Score</th>
      <th>imagegame, Quality Score (std)</th>
      <th>privateshared, % Played</th>
      <th>privateshared, Quality Score</th>
      <th>privateshared, Quality Score (std)</th>
      <th>referencegame, % Played</th>
      <th>referencegame, Quality Score</th>
      <th>referencegame, Quality Score (std)</th>
      <th>taboo, % Played</th>
      <th>taboo, Quality Score</th>
      <th>taboo, Quality Score (std)</th>
      <th>wordle, % Played</th>
      <th>wordle, Quality Score</th>
      <th>wordle, Quality Score (std)</th>
      <th>wordle_withclue, % Played</th>
      <th>wordle_withclue, Quality Score</th>
      <th>wordle_withclue, Quality Score (std)</th>
      <th>wordle_withcritic, % Played</th>
      <th>wordle_withcritic, Quality Score</th>
      <th>wordle_withcritic, Quality Score (std)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Llama-3-70B-Together.ai-t0.0--Llama-3-70B-Together.ai-t0.0</th>
      <td>35.20</td>
      <td>79.52</td>
      <td>44.26</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>100.0</td>
      <td>84.67</td>
      <td>13.39</td>
      <td>100.0</td>
      <td>65.00</td>
      <td>47.83</td>
      <td>93.33</td>
      <td>68.75</td>
      <td>39.19</td>
      <td>86.67</td>
      <td>3.85</td>
      <td>9.41</td>
      <td>93.33</td>
      <td>13.69</td>
      <td>32.41</td>
      <td>83.33</td>
      <td>29.60</td>
      <td>38.34</td>
    </tr>
    <tr>
      <th>Llama-3-70B-groq-t0.0--Llama-3-70B-groq-t0.0</th>
      <td>39.34</td>
      <td>82.35</td>
      <td>47.77</td>
      <td>10.0</td>
      <td>80.5</td>
      <td>14.06</td>
      <td>100.0</td>
      <td>75.49</td>
      <td>21.43</td>
      <td>100.0</td>
      <td>62.78</td>
      <td>48.47</td>
      <td>96.67</td>
      <td>68.97</td>
      <td>41.35</td>
      <td>83.33</td>
      <td>2.00</td>
      <td>7.07</td>
      <td>93.33</td>
      <td>17.38</td>
      <td>32.47</td>
      <td>93.10</td>
      <td>27.28</td>
      <td>41.61</td>
    </tr>
    <tr>
      <th>Llama-3-70b-Instruct-Anyscale-t0.0--Llama-3-70b-Instruct-Anyscale-t0.0</th>
      <td>34.26</td>
      <td>80.00</td>
      <td>42.82</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>100.0</td>
      <td>84.32</td>
      <td>13.82</td>
      <td>100.0</td>
      <td>63.89</td>
      <td>48.17</td>
      <td>93.33</td>
      <td>68.75</td>
      <td>39.19</td>
      <td>86.67</td>
      <td>1.92</td>
      <td>6.94</td>
      <td>96.67</td>
      <td>13.22</td>
      <td>31.93</td>
      <td>83.33</td>
      <td>24.80</td>
      <td>36.81</td>
    </tr>
    <tr>
      <th>Llama-3-8B-Together.ai-t0.0--Llama-3-8B-Together.ai-t0.0</th>
      <td>21.66</td>
      <td>74.67</td>
      <td>29.01</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>96.0</td>
      <td>59.22</td>
      <td>30.32</td>
      <td>100.0</td>
      <td>48.89</td>
      <td>50.13</td>
      <td>100.00</td>
      <td>36.67</td>
      <td>44.66</td>
      <td>93.33</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>76.67</td>
      <td>17.54</td>
      <td>34.99</td>
      <td>56.67</td>
      <td>11.76</td>
      <td>26.69</td>
    </tr>
    <tr>
      <th>Llama-3-8B-groq-t0.0--Llama-3-8B-groq-t0.0</th>
      <td>17.79</td>
      <td>77.43</td>
      <td>22.98</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>92.0</td>
      <td>41.67</td>
      <td>22.77</td>
      <td>100.0</td>
      <td>38.33</td>
      <td>48.76</td>
      <td>100.00</td>
      <td>35.00</td>
      <td>42.82</td>
      <td>96.67</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>93.33</td>
      <td>16.19</td>
      <td>35.58</td>
      <td>60.00</td>
      <td>6.67</td>
      <td>23.76</td>
    </tr>
    <tr>
      <th>Llama-3-8b-Instruct-Anyscale-t0.0--Llama-3-8b-Instruct-Anyscale-t0.0</th>
      <td>19.32</td>
      <td>75.81</td>
      <td>25.48</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>94.0</td>
      <td>59.96</td>
      <td>29.90</td>
      <td>100.0</td>
      <td>27.22</td>
      <td>44.63</td>
      <td>100.00</td>
      <td>38.33</td>
      <td>44.82</td>
      <td>93.33</td>
      <td>3.57</td>
      <td>13.11</td>
      <td>83.33</td>
      <td>14.80</td>
      <td>33.80</td>
      <td>60.00</td>
      <td>8.98</td>
      <td>24.07</td>
    </tr>
  </tbody>
</table>